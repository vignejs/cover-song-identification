{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vignejs/cover-song-identification/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL5hUmGqDQxA"
      },
      "source": [
        "# Siamese convolutional Neural Network for Cover song identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5r3R07OXeS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_g8hOPzW4Ju"
      },
      "source": [
        "!pip install tensorflow-addons==0.9.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkNC8_oi6bvp"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import pickle\n",
        "import itertools as it\n",
        "import pandas as pd\n",
        "from scipy.linalg import block_diag\n",
        "from scipy import interpolate\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, Layer, BatchNormalization, Lambda, ZeroPadding2D, ReLU, Average, Permute\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, Callback, CSVLogger\n",
        "from tensorflow.python.framework.ops import Tensor\n",
        "from typing import Tuple, List\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBkVghK34gJ8"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-49-RYS8Yf2"
      },
      "source": [
        "feature_type = \"chroma_cens\"\n",
        "spect_len = 500\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    root = Path(\"/content/drive/My Drive/project\")\n",
        "else:\n",
        "    root = Path.cwd()\n",
        "\n",
        "traindf = pd.HDFStore(root / 'datasets' / 'trainset.h5').select(feature_type)\n",
        "testdf = pd.HDFStore(root / 'datasets' / 'valset.h5').select(feature_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TkPmCKLu5e"
      },
      "source": [
        "train = traindf.values.reshape(1000, 13, 12, 500)\n",
        "test = testdf.values.reshape(5000, 2, 12, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkXHxtz3_51F"
      },
      "source": [
        "def gen_indexes(w_shape, t_shape):\n",
        "    indexes = list()\n",
        "    \n",
        "    p = list(it.permutations(range(t_shape), 2))\n",
        "    \n",
        "    r = range(w_shape)\n",
        "    for i in r:\n",
        "        rand = np.random.choice([x for x in r if x != i], size=len(p), replace=False)\n",
        "        for j, (x, y) in enumerate(p):\n",
        "            indexes.append([(i,x), (i,y)])\n",
        "            indexes.append([(i, x), (rand[j], y)])\n",
        "            \n",
        "    return indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NRuy6ztFO_r"
      },
      "source": [
        "start = time.time()\n",
        "indexes_train = gen_indexes(1000, 13)\n",
        "indexes_test = gen_indexes(5000, 2)\n",
        "print(time.time() - start)\n",
        "len(indexes_train), len(indexes_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqJnq6vm4xau"
      },
      "source": [
        "### Using keras.seqential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9sEzrGN4tLf"
      },
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    '''Generates data for Keras'''\n",
        "    def __init__(self, data, indexes, batch_size=100, dim=(12, 1000), n_channels=1, shuffle=False, val=False):\n",
        "        '''Initialization'''\n",
        "        self.dim = dim\n",
        "        self.data = data\n",
        "        self.val = val\n",
        "        self.n_channels = n_channels\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = indexes    \n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        '''Denotes the number of batches per epoch'''\n",
        "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''Generate one batch of data'''\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # return data\n",
        "        return self.__data_generation(indexes)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        '''Updates indexes after each epoch'''\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)   \n",
        "    \n",
        "    def __data_augmentor(self, a):\n",
        "        \n",
        "        rng = int(np.random.normal() * spect_len)\n",
        "        aug = np.roll(a, rng, axis=1)\n",
        "\n",
        "        rng = int(np.random.normal() * 12)\n",
        "        aug = np.roll(aug, rng, axis=0)\n",
        "        \n",
        "        times = np.arange(0, spect_len)\n",
        "        func = interpolate.interp1d(times, aug, kind='nearest', fill_value='extrapolate')\n",
        "            \n",
        "        if np.random.uniform() < 0.3:\n",
        "            \n",
        "            rng = 1 + np.random.normal()  # random number to determine the factor of time stretching\n",
        "\n",
        "            if 0.7 <= rng <= 1.3:\n",
        "                times = np.linspace(0, spect_len - 1, int(spect_len * rng))\n",
        "                aug = func(times)  # applying time stretching\n",
        "                \n",
        "        if np.random.uniform() < 0.3:\n",
        "            rng = np.random.uniform()  # random number to determine which operation to apply for time warping\n",
        "\n",
        "            if rng < 0.3:  # silence\n",
        "                # each frame has a probability of 0.1 to be silenced\n",
        "                silence_idxs = np.random.choice([False, True], size=times.size, p=[.9, .1])\n",
        "                aug[:, silence_idxs] = np.zeros((12, 1))\n",
        "\n",
        "            elif rng < 0.7:  # duplicate\n",
        "                # each frame has a probability of 0.15 to be duplicated\n",
        "                duplicate_idxs = np.random.choice([False, True], size=times.size, p=[.85, .15])\n",
        "                times = np.sort(np.concatenate((times, times[duplicate_idxs])))\n",
        "                aug = func(times)\n",
        "\n",
        "            else:  # remove\n",
        "                # each frame has a probability of 0.1 to be removed\n",
        "                remaining_idxs = np.random.choice([False, True], size=times.size, p=[.1, .9])\n",
        "                times = times[remaining_idxs]\n",
        "                aug = func(times)\n",
        "\n",
        "        \n",
        "        if aug.shape[1] > spect_len:\n",
        "            aug = aug[:, :spect_len]\n",
        "        else:\n",
        "            aug = np.pad(aug, ((0, 0), (0, spect_len - aug.shape[1])), 'wrap')   \n",
        "\n",
        "        return aug\n",
        "    \n",
        "    def _data_augmentor(self, a):\n",
        "        if self.val:\n",
        "            return a\n",
        "        else:\n",
        "            return self.__data_augmentor(a)\n",
        "\n",
        "    def __data_generation(self, indexes):\n",
        "        '''Generates data containing batch_size samples''' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X1 = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n",
        "        X2 = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n",
        "        Y = np.empty((self.batch_size, 1), dtype=np.int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, (x, y) in enumerate(indexes):\n",
        "            # sample sample\n",
        "            r1 = self.data[x]\n",
        "            r2 = self.data[y]\n",
        "\n",
        "            # augment sample\n",
        "            r1 = self._data_augmentor(r1)\n",
        "            r2 = self._data_augmentor(r2)\n",
        "\n",
        "            # store sample\n",
        "            X1[i, ] = r1.reshape(12, spect_len, 1)\n",
        "            X2[i, ] = r2.reshape(12, spect_len, 1)\n",
        "\n",
        "            # Store class\n",
        "            if x[0] == y[0]:\n",
        "                Y[i, ] = [0]\n",
        "            else:\n",
        "                Y[i, ] = [1]     \n",
        "\n",
        "        return [X1, X2], Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDxId9xJP4EL"
      },
      "source": [
        "### ADAMScheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otrg-aalOVAZ"
      },
      "source": [
        "class ADAMScheduler(Callback):\n",
        "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
        "    # Usage\n",
        "        ```python\n",
        "            schedule = SGDRScheduler(min_lr=1e-5,\n",
        "                                     max_lr=1e-2,\n",
        "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
        "                                     lr_decay=0.9,\n",
        "                                     cycle_length=5,\n",
        "                                     mult_factor=1.5)\n",
        "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
        "        ```\n",
        "    # Arguments\n",
        "        min_lr: The lower bound of the learning rate range for the experiment.\n",
        "        max_lr: The upper bound of the learning rate range for the experiment.\n",
        "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
        "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
        "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
        "        cycle_length: Initial number of epochs in a cycle.\n",
        "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
        "    # References\n",
        "        Blog post: jeremyjordan.me/nn-learning-rate\n",
        "        Original paper: http://arxiv.org/abs/1608.03983\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=10,\n",
        "                 mult_factor=2):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.learning_rate, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('learning_rate', []).append(K.get_value(self.model.optimizer.learning_rate))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, self.clr())\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.ep_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        print(f\"Current learning rate {K.get_value(self.model.optimizer.learning_rate)}\")\n",
        "        print(f\"Elapsed time : {time.time() - self.ep_start:.2f}s\")\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            print('Restarts')\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "            self.model.save(root / \"weights/weights.best-{:.2e}-epoch-{:05d}.hdf5\".format(self.model.count_params(), epoch + 1))\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duECInUZt758"
      },
      "source": [
        "### Dual CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe2BSM1cC1he"
      },
      "source": [
        "def squared_differences(pair_of_tensors):\n",
        "    x, y = pair_of_tensors\n",
        "    return K.square(x - y)\n",
        "\n",
        "def l2_norm(x, axis=None):\n",
        "    \"\"\"\n",
        "    takes an input tensor and returns the l2 norm along specified axis\n",
        "    \"\"\"\n",
        "\n",
        "    square_sum = K.sum(K.square(x), axis=axis, keepdims=True)\n",
        "    norm = K.sqrt(K.maximum(square_sum, K.epsilon()))\n",
        "\n",
        "    return norm\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1) \n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
        "\n",
        "def W_init(shape, dtype=None):\n",
        "    \"\"\"Initialize weights as in paper\"\"\"\n",
        "    values = np.random.normal(loc=0, scale=1e-2, size=shape)\n",
        "    return K.variable(values, dtype=dtype)\n",
        "\n",
        "def W_init_dense(shape, dtype=None):\n",
        "    \"\"\"Initialize weights as in paper\"\"\"\n",
        "    values = np.random.normal(loc=0, scale=2e-1, size=shape)\n",
        "    return K.variable(values, dtype=dtype)\n",
        " \n",
        "def b_init(shape, dtype=None):\n",
        "    \"\"\"Initialize bias as in paper\"\"\"\n",
        "    values = np.random.normal(loc=0.5, scale=1e-2, size=shape)\n",
        "    return K.variable(values, dtype=dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52JINqUwDpar"
      },
      "source": [
        "def create_base_network(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D(padding=(3, 0), input_shape=input_shape))\n",
        "    model.add(Conv2D(32, (7, 7), strides=(1, 3), kernel_initializer=W_init, bias_initializer=b_init))\n",
        "    model.add(ReLU())\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(ZeroPadding2D(padding=(2, 0)))\n",
        "    model.add(Conv2D(32, (5, 5), strides=(1, 2), kernel_initializer=W_init, bias_initializer=b_init))\n",
        "    model.add(ReLU())\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(ZeroPadding2D(padding=(2, 0)))\n",
        "    model.add(Conv2D(32, (5, 5), strides=(1, 2), kernel_initializer=W_init, bias_initializer=b_init))\n",
        "    model.add(ReLU())\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(ZeroPadding2D(padding=(2, 0)))\n",
        "    model.add(Conv2D(32, (5, 5), strides=(2, 2), kernel_initializer=W_init, bias_initializer=b_init))\n",
        "    model.add(ReLU())\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='sigmoid', kernel_initializer=W_init_dense, bias_initializer=b_init))\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_twin():\n",
        "    input_shape = (12, spect_len, 1)\n",
        "    original_input = Input(shape=input_shape, name=\"input_1\")\n",
        "    cover_input = Input(shape=input_shape, name=\"input_2\")\n",
        "    model = create_base_network(input_shape)\n",
        "    original_model = model(original_input)\n",
        "    cover_model = model(cover_input)\n",
        "\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([original_model, cover_model])\n",
        "\n",
        "    output = Dense(1, activation='sigmoid')(L1_distance)\n",
        "\n",
        "    model = Model(inputs=[original_input, cover_input], outputs=[output])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_twin()\n",
        "best_weights_file = \"{}/weights/weights.best-{:.2e}.hdf5\".format(root, model.count_params())\n",
        "\n",
        "opt = tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-2)\n",
        "\n",
        "custom_objects = {\"contrastive_loss\": contrastive_loss,\n",
        "                  \"acc\": acc,\n",
        "                  \"W_init\": W_init,\n",
        "                  \"W_init_dense\": W_init_dense,\n",
        "                  \"b_init\": b_init}\n",
        "\n",
        "# model = load_model(best_weights_file, custom_objects=custom_objects)\n",
        "model.compile(loss=contrastive_loss, optimizer=opt, metrics=[acc])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_gT2WahJcfG"
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (12, spect_len),\n",
        "          'batch_size': 128,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "training_generator = DataGenerator(train, indexes_train, **params)\n",
        "validation_generator = DataGenerator(test, indexes_test, **params, val=True)\n",
        "\n",
        "def tr_generator():\n",
        "    multi_enqueuer = tf.keras.utils.OrderedEnqueuer(training_generator, use_multiprocessing=True, shuffle=False)\n",
        "    multi_enqueuer.start(workers=4, max_queue_size=5)\n",
        "    while True:\n",
        "        yield next(multi_enqueuer.get())\n",
        "\n",
        "def va_generator():\n",
        "    multi_enqueuer = tf.keras.utils.OrderedEnqueuer(validation_generator, use_multiprocessing=True, shuffle=False)\n",
        "    multi_enqueuer.start(workers=4, max_queue_size=5)\n",
        "    while True:\n",
        "        yield next(multi_enqueuer.get())\n",
        "\n",
        "\n",
        "training_dataset = tr_generator()\n",
        "validation_dataset = va_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEIEPmuYuDEy"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNnyMuVRKJDO"
      },
      "source": [
        "checkpoint = ModelCheckpoint(best_weights_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "def step_decay(epoch):\n",
        "    initial_lrate = 0.1\n",
        "    drop = 0.99\n",
        "    initial_lrate = initial_lrate * np.power(0.9999, epoch // 10)\n",
        "    lrate = np.maximum(initial_lrate * np.power(drop, epoch % 10), 0.1 * np.power(0.99, 9))\n",
        "    return lrate\n",
        "\n",
        "csv_logger = CSVLogger(f\"{root}/logs/training-{model.count_params():.2e}.csv\", append=False)\n",
        "\n",
        "lr_schedule = ADAMScheduler(min_lr=0,\n",
        "                            max_lr=1e-2,\n",
        "                            steps_per_epoch=len(training_generator),\n",
        "                            lr_decay=0.9,\n",
        "                            cycle_length=50,\n",
        "                            mult_factor=1)\n",
        "\n",
        "callbacks = [checkpoint, lr_schedule, csv_logger]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvk3bfpCq8ya",
        "scrolled": true
      },
      "source": [
        "# Train model on dataset (sequence)\n",
        "history = model.fit(training_dataset,\n",
        "                    validation_data=validation_dataset,\n",
        "                    steps_per_epoch=len(training_generator),\n",
        "                    validation_steps=len(validation_generator),\n",
        "                    use_multiprocessing=False,\n",
        "                    workers=1,\n",
        "                    verbose=0,\n",
        "                    callbacks=callbacks,\n",
        "                    epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ub6x30OnGRC"
      },
      "source": [
        "accuracy = history.history['acc']\n",
        "val_accuracy = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}